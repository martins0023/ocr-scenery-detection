{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Character recognition\n",
    "\n",
    "<b>Optical character recognition (OCR) is process of classification of opti-\n",
    "cal patterns contained in a digital image. The character recognition is achieved\n",
    "through segmentation, feature extraction and classification.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stages of Optical Character Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stages of the Character Recognition\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Stages of the Character Recognition\")\n",
    "image_path = 'images/steps/image.png' #this image shows the Stages of the process\n",
    "image_orignal=cv2.imread(image_path) #read the image path\n",
    "plt.imshow(image_orignal)\n",
    "plt.axis(\"off\")\n",
    "plt.show() #display original image that shows the steps\n",
    "#Image(filename=r'image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Segmentation\n",
    "\n",
    "The next OCR component is location segmentation. Segmentation determines constituents\n",
    "of an image. It is necessary to locate regions of document which have\n",
    "printed data and are distinguished from figures and graphics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word recogntion through OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m kernel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m),np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     10\u001b[0m dilated \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdilate(thresh1, kernel, iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m _,contours, hierarchy \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfindContours(dilated,cv2\u001b[38;5;241m.\u001b[39mRETR_TREE,cv2\u001b[38;5;241m.\u001b[39mCHAIN_APPROX_SIMPLE)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cnt \u001b[38;5;129;01min\u001b[39;00m contours:\n\u001b[1;32m     14\u001b[0m     x,y,w,h \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mboundingRect(cnt)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "file =  r'sample.jpg'\n",
    "im1 = cv2.imread(file,0)\n",
    "im = cv2.imread(file)\n",
    "\n",
    "ret,thresh1 = cv2.threshold(im1,180,255,cv2.THRESH_BINARY_INV)\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "dilated = cv2.dilate(thresh1, kernel, iterations = 2)\n",
    "_,contours, hierarchy = cv2.findContours(dilated,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "for cnt in contours:\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    #bound the images\n",
    "    cv2.rectangle(im,(x,y),(x+w,y+h),(0,255,0),1)\n",
    "\n",
    "cv2.namedWindow('scenery detection', cv2.WINDOW_NORMAL)\n",
    "cv2.imwrite('BindingBox4.jpg',im)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='sample.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing classes ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'noOfClasses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                 classNames\u001b[38;5;241m.\u001b[39mappend(classes[x])     \n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mpreProcessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36mpreProcessing\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreProcessing\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImporting classes ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[43mnoOfClasses\u001b[49m):\n\u001b[1;32m      5\u001b[0m         digitList \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(x) )\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m digitList:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noOfClasses' is not defined"
     ]
    }
   ],
   "source": [
    "def preProcessing():\n",
    "    print(\"Importing classes ...\")\n",
    "\n",
    "    for x in range(0, noOfClasses):\n",
    "        digitList = os.listdir(path + \"/\" + str(x) )\n",
    "    \n",
    "        for y in digitList:\n",
    "            imagePath = path + \"/\" + str(x) + \"/\" +  str(y)\n",
    "                \n",
    "            currentImage = cv2.imread(imagePath)    \n",
    "            currentImage = cv2.cvtColor(currentImage, cv2.COLOR_BGR2GRAY)    \n",
    "            currentImage=crop_img(currentImage)\n",
    "\n",
    "            # check the image is empty or not\n",
    "            if(len(currentImage)==1):\n",
    "                print(\"image do not have information: \",imagePath)\n",
    "            else:\n",
    "                currentImage = cv2.resize(currentImage, (28,28))  \n",
    "                currentImage = currentImage/255\n",
    "                images.append(currentImage)\n",
    "                classNo.append(x) \n",
    "                classNames.append(classes[x])     \n",
    "        \n",
    "        \n",
    "        print(x, end=\" \")\n",
    "preProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import importlib\n",
    "from .utils import CTCLabelConverter\n",
    "import math\n",
    "\n",
    "def custom_mean(x):\n",
    "    return x.prod()**(2.0/np.sqrt(len(x)))\n",
    "\n",
    "def contrast_grey(img):\n",
    "    high = np.percentile(img, 90)\n",
    "    low  = np.percentile(img, 10)\n",
    "    return (high-low)/np.maximum(10, high+low), high, low\n",
    "\n",
    "def adjust_contrast_grey(img, target = 0.4):\n",
    "    contrast, high, low = contrast_grey(img)\n",
    "    if contrast < target:\n",
    "        img = img.astype(int)\n",
    "        ratio = 200./np.maximum(10, high-low)\n",
    "        img = (img - low + 25)*ratio\n",
    "        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "class NormalizePAD(object):\n",
    "\n",
    "    def __init__(self, max_size, PAD_type='right'):\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.max_size = max_size\n",
    "        self.max_width_half = math.floor(max_size[2] / 2)\n",
    "        self.PAD_type = PAD_type\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        c, h, w = img.size()\n",
    "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
    "        Pad_img[:, :, :w] = img  # right pad\n",
    "        if self.max_size[2] != w:  # add border Pad\n",
    "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
    "\n",
    "        return Pad_img\n",
    "\n",
    "class ListDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, image_list):\n",
    "        self.image_list = image_list\n",
    "        self.nSamples = len(image_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.image_list[index]\n",
    "        return Image.fromarray(img, 'L')\n",
    "\n",
    "class AlignCollate(object):\n",
    "\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, adjust_contrast = 0.):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "        self.adjust_contrast = adjust_contrast\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        images = batch\n",
    "\n",
    "        resized_max_w = self.imgW\n",
    "        input_channel = 1\n",
    "        transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "        resized_images = []\n",
    "        for image in images:\n",
    "            w, h = image.size\n",
    "            #### augmentation here - change contrast\n",
    "            if self.adjust_contrast > 0:\n",
    "                image = np.array(image.convert(\"L\"))\n",
    "                image = adjust_contrast_grey(image, target = self.adjust_contrast)\n",
    "                image = Image.fromarray(image, 'L')\n",
    "\n",
    "            ratio = w / float(h)\n",
    "            if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                resized_w = self.imgW\n",
    "            else:\n",
    "                resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "            resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
    "            resized_images.append(transform(resized_image))\n",
    "\n",
    "        image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
    "        return image_tensors\n",
    "\n",
    "def recognizer_predict(model, converter, test_loader, batch_max_length,\\\n",
    "                       ignore_idx, char_group_idx, decoder = 'greedy', beamWidth= 5, device = 'cpu'):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for image_tensors in test_loader:\n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "            # For max length prediction\n",
    "            length_for_pred = torch.IntTensor([batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "            preds = model(image, text_for_pred)\n",
    "\n",
    "            # Select max probabilty (greedy decoding) then decode index to character\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "\n",
    "            ######## filter ignore_char, rebalance\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_prob = preds_prob.cpu().detach().numpy()\n",
    "            preds_prob[:,:,ignore_idx] = 0.\n",
    "            pred_norm = preds_prob.sum(axis=2)\n",
    "            preds_prob = preds_prob/np.expand_dims(pred_norm, axis=-1)\n",
    "            preds_prob = torch.from_numpy(preds_prob).float().to(device)\n",
    "\n",
    "            if decoder == 'greedy':\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds_prob.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode_greedy(preds_index.data.cpu().detach().numpy(), preds_size.data)\n",
    "            elif decoder == 'beamsearch':\n",
    "                k = preds_prob.cpu().detach().numpy()\n",
    "                preds_str = converter.decode_beamsearch(k, beamWidth=beamWidth)\n",
    "            elif decoder == 'wordbeamsearch':\n",
    "                k = preds_prob.cpu().detach().numpy()\n",
    "                preds_str = converter.decode_wordbeamsearch(k, beamWidth=beamWidth)\n",
    "\n",
    "            preds_prob = preds_prob.cpu().detach().numpy()\n",
    "            values = preds_prob.max(axis=2)\n",
    "            indices = preds_prob.argmax(axis=2)\n",
    "            preds_max_prob = []\n",
    "            for v,i in zip(values, indices):\n",
    "                max_probs = v[i!=0]\n",
    "                if len(max_probs)>0:\n",
    "                    preds_max_prob.append(max_probs)\n",
    "                else:\n",
    "                    preds_max_prob.append(np.array([0]))\n",
    "\n",
    "            for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "                confidence_score = custom_mean(pred_max_prob)\n",
    "                result.append([pred, confidence_score])\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_recognizer(recog_network, network_params, character,\\\n",
    "                   separator_list, dict_list, model_path,\\\n",
    "                   device = 'cpu', quantize = True):\n",
    "\n",
    "    converter = CTCLabelConverter(character, separator_list, dict_list)\n",
    "    num_class = len(converter.character)\n",
    "\n",
    "    if recog_network == 'generation1':\n",
    "        model_pkg = importlib.import_module(\"easyocr.model.model\")\n",
    "    elif recog_network == 'generation2':\n",
    "        model_pkg = importlib.import_module(\"easyocr.model.vgg_model\")\n",
    "    else:\n",
    "        model_pkg = importlib.import_module(recog_network)\n",
    "    model = model_pkg.Model(num_class=num_class, **network_params)\n",
    "\n",
    "    if device == 'cpu':\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for key, value in state_dict.items():\n",
    "            new_key = key[7:]\n",
    "            new_state_dict[new_key] = value\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        if quantize:\n",
    "            try:\n",
    "                torch.quantization.quantize_dynamic(model, dtype=torch.qint8, inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    return model, converter\n",
    "\n",
    "def get_text(character, imgH, imgW, recognizer, converter, image_list,\\\n",
    "             ignore_char = '',decoder = 'greedy', beamWidth =5, batch_size=1, contrast_ths=0.1,\\\n",
    "             adjust_contrast=0.5, filter_ths = 0.003, workers = 1, device = 'cpu'):\n",
    "    batch_max_length = int(imgW/10)\n",
    "\n",
    "    char_group_idx = {}\n",
    "    ignore_idx = []\n",
    "    for char in ignore_char:\n",
    "        try: ignore_idx.append(character.index(char)+1)\n",
    "        except: pass\n",
    "\n",
    "    coord = [item[0] for item in image_list]\n",
    "    img_list = [item[1] for item in image_list]\n",
    "    AlignCollate_normal = AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True)\n",
    "    test_data = ListDataset(img_list)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=int(workers), collate_fn=AlignCollate_normal, pin_memory=True)\n",
    "\n",
    "    # predict first round\n",
    "    result1 = recognizer_predict(recognizer, converter, test_loader,batch_max_length,\\\n",
    "                                 ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "\n",
    "    # predict second round\n",
    "    low_confident_idx = [i for i,item in enumerate(result1) if (item[1] < contrast_ths)]\n",
    "    if len(low_confident_idx) > 0:\n",
    "        img_list2 = [img_list[i] for i in low_confident_idx]\n",
    "        AlignCollate_contrast = AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True, adjust_contrast=adjust_contrast)\n",
    "        test_data = ListDataset(img_list2)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "                        test_data, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=int(workers), collate_fn=AlignCollate_contrast, pin_memory=True)\n",
    "        result2 = recognizer_predict(recognizer, converter, test_loader, batch_max_length,\\\n",
    "                                     ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "\n",
    "    result = []\n",
    "    for i, zipped in enumerate(zip(coord, result1)):\n",
    "        box, pred1 = zipped\n",
    "        if i in low_confident_idx:\n",
    "            pred2 = result2[low_confident_idx.index(i)]\n",
    "            if pred1[1]>pred2[1]:\n",
    "                result.append( (box, pred1[0], pred1[1]) )\n",
    "            else:\n",
    "                result.append( (box, pred2[0], pred2[1]) )\n",
    "        else:\n",
    "            result.append( (box, pred1[0], pred1[1]) )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Scanning\n",
    "\n",
    "In OCR optical scanners are used which consist of transport mechanism and sensing device that converts light intensity\n",
    "into grey levels. Printed documents consist of black print on white background.\n",
    "When performing OCR multilevel image is converted into bi-level black and white\n",
    "image. This process known as thresholding is performed on scanner to save memory\n",
    "space and computational effort. The thresholding process is important as the\n",
    "results of recognition are totally dependent on quality of bi-level image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character recogntion through OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "#\n",
    "file = r'output2/sample.jpg'\n",
    "#file = r'/home/naga/Documents/Naga/Machine Learning/Data_extract/data/1 (3).jpg'\n",
    "im1 = cv2.imread(file,0)\n",
    "im = cv2.imread(file)\n",
    "ret,thresh1 = cv2.threshold(im1,180,278,cv2.THRESH_BINARY)\n",
    "_,contours, hierarchy = cv2.findContours(thresh1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "for cnt in contours:\n",
    "\tx,y,w,h = cv2.boundingRect(cnt)\n",
    "\t#bound the images\n",
    "\tcv2.rectangle(im,(x,y),(x+w,y+h),(0,255,0),1)\n",
    "i=0\n",
    "for cnt in contours:\n",
    "\tx,y,w,h = cv2.boundingRect(cnt)\n",
    "\t#following if statement is to ignore the noises and save the images which are of normal size(character)\n",
    "\t#In order to write more general code, than specifying the dimensions as 100,\n",
    "\t# number of characters should be divided by word dimension\n",
    "\tif w>100 and h>100:\n",
    "\t\t#save individual images\n",
    "\t\tcv2.imwrite(str(i)+\".jpg\",thresh1[y:y+h,x:x+w])\n",
    "\t\ti=i+1\n",
    "cv2.namedWindow('BindingBox', cv2.WINDOW_NORMAL)\n",
    "cv2.imwrite('output2/BindingBox3.jpg',im)\n",
    "from IPython.display import Image\n",
    "Image(filename='output2/BindingBox3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters recognised through pytesseract (creating boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m boxes:\n\u001b[1;32m     11\u001b[0m     b \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mrectangle(img, (\u001b[38;5;28mint\u001b[39m(\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m), h \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m(b[\u001b[38;5;241m2\u001b[39m])), (\u001b[38;5;28mint\u001b[39m(b[\u001b[38;5;241m3\u001b[39m]), h \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m(b[\u001b[38;5;241m4\u001b[39m])), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# show annotated image and wait for keypress\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# cv2.imshow(files[0], img)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexboxe.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m,img)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# read the image and get the dimensions\n",
    "img = cv2.imread(file)\n",
    "h, w, _ = img.shape # assumes color image\n",
    "\n",
    "# run tesseract, returning the bounding boxes\n",
    "boxes = pytesseract.image_to_boxes(img).split('\\n') # \n",
    "# Box = list(map(lambda box:(box[:1],list(map(int,box[2:][:-2].split(' ')))),boxes))\n",
    "\n",
    "# draw the bounding boxes on the image\n",
    "for b in boxes:\n",
    "    b = b.split(' ')\n",
    "    img = cv2.rectangle(img, (int(b[1]), h - int(b[2])), (int(b[3]), h - int(b[4])), (0, 255, 0), 2)\n",
    "# show annotated image and wait for keypress\n",
    "# cv2.imshow(files[0], img)\n",
    "\n",
    "cv2.imwrite('exboxe.jpg',img)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='exboxe.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1, 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contours[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "# For each contour, find the bounding rectangle and draw it\n",
    "count = 0\n",
    "for component in zip(contours, hierarchy[0]):\n",
    "    currentContour = component[0]\n",
    "    currentHierarchy = component[1]\n",
    "    x,y,w,h = cv2.boundingRect(currentContour)\n",
    "    if currentHierarchy[3] <= 0:\n",
    "        # these are the outermost parent components\n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),(0,0,128),1)\n",
    "        count = count + 1\n",
    "        letter = im[y:y+h,x:x+w]\n",
    "        cv2.imwrite('output2/words/l-{}.jpg'.format(count),letter)\n",
    "#     elif currentHierarchy[2] < 0:\n",
    "#         # these are the innermost child components\n",
    "#         cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "\n",
    "# Finally show the image\n",
    "cv2.imwrite('img.jpg',im)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "1) The raw data depending on the data\n",
    "acquisition type is subjected to a number of preliminary processing steps to make it\n",
    "usable in the descriptive stages of character analysis. The image resulting from scanning\n",
    "process may contain certain amount of noise\n",
    "\n",
    "2) Smoothing implies both filling\n",
    "and thinning. Filling eliminates small breaks, gaps and holes in digitized characters\n",
    "while thinning reduces width of line.\n",
    "\n",
    "    (a) noise reduction\n",
    "\n",
    "    (b) normalization of the data and\n",
    "\n",
    "    (c) compression in the amount of information to be retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.  Noise Reduction\n",
    "Noise reduction: The noise introduced by the optical scanning device or the\n",
    "writing instrument causes disconnected line segments, bumps and gaps in\n",
    "lines, filled loops, etc. The distortion including local variations, rounding of\n",
    "corners, dilation and erosion is a potential problem.\n",
    "    \n",
    "    (i) filtering\n",
    "    (ii) morphological operations and \n",
    "    (iii) noise modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Normalization: \n",
    "The normalization methods aim to remove the variations of the\n",
    "writing and obtain standardized data. Some of the commonly used methods\n",
    "for normalization are \n",
    "    \n",
    "    (i) skew normalization and baseline extraction\n",
    "    (ii) slant normalization \n",
    "    (iii) size normalization and \n",
    "    (iv) contour smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading label for all image data\n",
    "letters = [j[:-4][-1] for j in os.listdir(r'data\\words')]\n",
    "letter_dict = dict([(k,[j for j in os.listdir(r'data\\words') if j[:-4][-1] ==k])  for k in Counter(letters).keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encod_y = dict(zip(np.unique(letters),np.arange(len(np.unique(letters)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in letter_dict['.']:\n",
    "    shutil.copyfile(r'data\\words\\\\'+i,r'data\\letters\\check\\\\'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copying each label data in specific folder name by creating\n",
    "# import shutil\n",
    "# for i in letter_dict.keys():\n",
    "#     if not os.path.exists(r'data\\letters\\\\'+i):\n",
    "#         os.mkdir(r'data\\letters\\\\'+i)\n",
    "#     for j in letter_dict[i]:\n",
    "#         shutil.copyfile(r'data\\words\\\\'+j,r'data\\letters\\\\'+i+'\\\\'+j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Duplicating the files with different names for each less than 10 number\n",
    "# to_copy = [(i,j) for i,j in Counter(letters).items() if j < 50]\n",
    "# for key in to_copy:\n",
    "#     for let in letter_dict[key[0]]:\n",
    "#         [shutil.copyfile(r'data\\words\\\\'+let,r'data\\words\\\\l-'+str(co)+'-l'+key[0]+'.jpg') for co in np.arange(4010,4060)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data_y = []\n",
    "for key in letter_dict.keys():\n",
    "    for let in letter_dict[key]:\n",
    "        im = PIL.Image.open(os.path.join(r'data\\words',let))\n",
    "        data.append(np.array(im.resize([32,32],PIL.Image.ANTIALIAS)))\n",
    "        data_y.append(encod_y[key])\n",
    "        \n",
    "#Image.fromarray(da1[345])\n",
    "data = np.array(data).reshape((len(data),32,32,3))\n",
    "data_y = np.array(data_y).reshape(len(data_y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shfl = np.arange(len(data))\n",
    "np.random.shuffle(shfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = data[shfl[:int(len(data)*0.8)]],data_y[shfl[:int(len(data)*0.8)]]\n",
    "X_test,y_test = data[shfl[int(len(data)*0.8):]],data_y[shfl[int(len(data)*0.8):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 376\n",
    "for i in encod_y.keys():\n",
    "    if encod_y[i]==data_y[n][0]:\n",
    "        print(i)\n",
    "PIL.Image.fromarray(data[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in encod_y.keys():\n",
    "    if encod_y[i]==y_train[n][0]:\n",
    "        print(i)\n",
    "PIL.Image.fromarray(X_train[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(details):\n",
    "    \"\"\"\n",
    "    This function take one argument as\n",
    "    input.This function will arrange\n",
    "    resulted text into proper format.\n",
    "    :param details: dictionary\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    parse_text = []\n",
    "    word_list = []\n",
    "    last_word = ''\n",
    "    for word in details['text']:\n",
    "        if word != '':\n",
    "            word_list.append(word)\n",
    "            last_word = word\n",
    "        if (last_word != '' and word == '') or (word == details['text'][-1]):\n",
    "            parse_text.append(word_list)\n",
    "            word_list = []\n",
    "\n",
    "    return parse_text\n",
    "\n",
    "\n",
    "def write_text(formatted_text):\n",
    "    \"\"\"\n",
    "    This function take one argument.\n",
    "    it will write arranged text into\n",
    "    a file.\n",
    "    :param formatted_text: list\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with open('resulted_text.txt', 'w', newline=\"\") as file:\n",
    "        csv.writer(file, delimiter=\" \").writerows(formatted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess or reduce the dimenstions using PCA\n",
    "Steps involved in the PCA\n",
    "\n",
    "Here is the short summary of the required steps:\n",
    "\n",
    "    Standardize the dataset  our dataset has 784 features and a total of 4833\n",
    "    Calculate the covariance matrix for the features in the dataset\n",
    "    Calculate the eigenvalues and eigenvectors for the covariance matrix\n",
    "    Pick K eigenvalues and form eigenvectors\n",
    "    Transform the original matrix\n",
    "    Reconstruction from Principal Component\n",
    "    Visualize the data\n",
    "\n",
    "Standardize the dataset:\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance. The standard score of a sample x is calculated as:\n",
    "=()\n",
    "where  is the mean of the training samples or zero if with_mean=False, and  is the standard deviation of the training samples or one if with_std=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ = X.mean(axis=0)\n",
    "\n",
    "# print(\"mean\",mean_)\n",
    "std_ = X.std(axis=0)\n",
    "# print(\"std\",std_)\n",
    "\n",
    "X_scaled = (X-mean_)/std_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Covariance Matrix:\n",
    "\n",
    "Variance reports variation of a single random variable, and covariance reports how much two random variables vary. On the diagonal of the covariance matrix we have variances, and other elements are the covariances. Covariance matrix has larger diagonal values(variance) and non diagonal values (covariance) are very small.\n",
    "\n",
    "The formula to calculate the covariance matrix:\n",
    "(,)==1()()1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of X_scaled: {X_scaled.shape}')\n",
    "features = X_scaled.T\n",
    "print(f'Shape of features: {features.shape}')\n",
    "cov_matrix = np.cov(features)\n",
    "print(f'Shape of cov_matrix: {cov_matrix.shape}')\n",
    "print('\\nSneak Peak of the covariance matrix:\\n')\n",
    "cov_matrix[0:4, 0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the eigenvalues and eigenvectors\n",
    "\n",
    "Eigendecomposition is a process that decomposes a square matrix into eigenvectors and eigenvalues. Eigenvectors are simple unit vectors, and eigenvalues are coefficients which give the magnitude to the eigenvectors.\n",
    "\n",
    "It turns out, eigenvectors of symmetric matrices are orthogonal. For PCA this means that we have the first principal component which explains most of the variance. Orthogonal to that is the second principal component, which explains most of the remaining variance. This is repeated for N number of principal components, where N equals to number of original features.\n",
    "\n",
    "And this turns out to be neat for us  principal components are sorted by percentage of variance explained, as we can decide how many should we keep. For example, if we have 100 features originally, but the first 3 principal components explain 95% of the variance, then it makes sense to keep only these 3 for visualizations and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_values, eig_vectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "\n",
    "\n",
    "print(f'First 10 eigenvalues: {eig_values[:10]}')\n",
    "print(f'\\n\\nLast 10 eigenvalues: {eig_values[-10:]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.stem(eig_values[:200], use_line_collection = True)\n",
    "plt.xlabel('Eigen value index')\n",
    "plt.ylabel('Eigen value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    exp_var = np.sum(eig_values[:i+1])*100 / np.sum(eig_values)\n",
    "    print(f'Eigenvectors upto {i+1} expresses {exp_var} % variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations\n",
    "Eigenvector Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance Matrix\n",
    "X_scaled[0].reshape((-1, 784)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigen vectors\n",
    "eig_vectors[:, 0].reshape((784, -1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick K eigenvalues and form eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K= [2,3,5,10,20,50,100,200,300,500,700,780]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pca = []\n",
    "for k in K:\n",
    "    result = np.dot(X_scaled.reshape((-1, 784)), eig_vectors[:,:k].reshape((784, -1)))\n",
    "    result_pca.append(result)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_1 = X_scaled.dot(eig_vectors.T[0])\n",
    "projected_2 = X_scaled.dot(eig_vectors.T[1])\n",
    "res2d = pd.DataFrame(projected_1, columns=['PC1'])\n",
    "res2d['PC2'] = projected_2\n",
    "res2d['Y'] = classNames\n",
    "res2d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#CNN model \n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import keras_metrics\n",
    "K.set_image_dim_ordering('th')\n",
    "import tensorflow as tf\n",
    "# Plot ad hoc CIFAR10 instances\n",
    "from keras.datasets import cifar10\n",
    "from matplotlib import pyplot\n",
    "from scipy.misc import toimage\n",
    "\n",
    "\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "@as_keras_metric\n",
    "def auc_pr(y_true, y_pred, curve='PR'):\n",
    "    return tf.metrics.auc(y_true, y_pred, curve=curve)\n",
    "\n",
    "# load data\n",
    "#(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_unq, y_train_ids = np.unique(y_train, return_inverse=True)\n",
    "y_test_unq,y_test_ids  = np.unique(y_test, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.np_utils.to_categorical(y_train_ids,len(y_train_unq))\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test_ids,len(y_test_unq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image.py:1213: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3 or 4 channels on axis 1. However, it was passed an array with shape (2218, 32, 32, 3) (32 channels).\n",
      "  ' channels).')\n"
     ]
    }
   ],
   "source": [
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 3)         9248      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 3)         9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 16, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 41)                21033     \n",
      "=================================================================\n",
      "Total params: 302,185\n",
      "Trainable params: 302,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(32, 32,3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(41, activation='softmax'))\n",
    "# Compile model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=sgd, \n",
    "#               metrics=['categorical_accuracy',keras_metrics.precision(), keras_metrics.recall()])\n",
    "class_value = 1\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics = [auc_pr]) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),validation_data=(X_test,y_test),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=50)\n",
    "\n",
    "# # here's a more \"manual\" example\n",
    "# for e in range(epochs):\n",
    "#     print('Epoch', e)\n",
    "#     batches = 0\n",
    "#     for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
    "#         model.fit(x_batch, y_batch)\n",
    "#         batches += 1\n",
    "#         if batches >= len(x_train) / 32:\n",
    "#             # we need to break the loop by hand because\n",
    "#             # the generator loops indefinitely\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-24b32369da7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_code\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_code' is not defined"
     ]
    }
   ],
   "source": [
    "y_.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 29, 26, 29, 33, 29, 29, 26, 26, 26, 26, 26, 26, 39, 26, 29, 26,\n",
       "       26, 26, 26, 26, 29, 33, 29, 26, 26, 29, 29, 26, 26, 26, 29, 26, 29,\n",
       "       39, 26, 29, 26, 26, 29, 26, 26,  1, 26, 26, 26, 26, 29, 26, 29, 29,\n",
       "       26, 26, 29, 26, 29, 26, 26, 33, 26, 26, 29, 26, 26, 29, 26, 33, 26,\n",
       "       26, 26, 26, 26, 39, 26, 26, 26, 29, 26, 29, 26, 29, 29, 26, 29, 26,\n",
       "       26, 26, 26, 26, 26, 29, 26,  1, 26, 26, 29, 26, 29, 26, 26, 26, 29,\n",
       "       26, 26, 26, 29, 26, 29, 26, 26, 29, 26, 29, 26, 26, 29, 26, 26, 26,\n",
       "       26, 29, 26, 26, 29, 26, 26, 26, 26, 26, 29, 26, 26, 26, 26, 29, 29,\n",
       "       26, 29, 26, 26, 26, 26, 26, 29, 29, 26, 29, 26, 26, 29, 26, 26, 26,\n",
       "       26,  1, 26, 29, 26, 26, 26, 29, 26, 26, 26, 26, 26, 29, 29, 29, 29,\n",
       "       29, 26, 26, 29, 26, 26, 29, 29, 26, 26, 29, 29, 26, 26, 29, 26, 26,\n",
       "       29, 33,  1, 29, 33, 29, 26, 26, 33, 26, 29, 26, 39, 26, 26, 26, 26,\n",
       "       29, 33, 26, 26, 26,  1, 39, 26,  1, 29, 33, 26, 26, 26, 26, 26, 33,\n",
       "       26, 26, 29, 26, 29,  1, 26, 33, 26, 29, 26, 29, 26, 29, 33, 26, 26,\n",
       "       26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 29, 29, 26, 26, 26,\n",
       "       29, 26, 26, 26, 26, 29, 26, 26, 26, 26, 26, 29, 26, 29, 26, 26, 26,\n",
       "       39, 29, 29, 26, 26, 26, 26, 33, 26,  1, 26, 26, 29, 39, 29, 26, 26,\n",
       "       26, 29, 29, 29, 26, 26, 26, 26, 33, 26, 29, 29, 29, 26, 26, 26, 26,\n",
       "       26, 26, 39, 29, 26, 26, 26,  1, 26, 26, 26, 26, 26, 26, 29, 26, 26,\n",
       "        1, 26], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, ids = np.unique(y_train, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 20,  4, ..., 32, 34, 10], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.np_utils.normalize(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Rand'] = np.random.uniform(0,1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using KNN\n",
    "\n",
    "Used k-nearest neighbor (KNN) to locate the nearby projections made by the training images, to do this\n",
    "\n",
    "    Step 1: Calculate Euclidean Distance-We can calculate the straight line distance between two vectors using the Euclidean distance measure. It is calculated as the square root of the sum of the squared differences between the two vectors.\n",
    "    ==1(12)2\n",
    "\n",
    "Step 2: Get Nearest Neighbors - Neighbors for a new piece of data in the dataset are the k closest instances(for our case k=5), as defined by our distance measure. To locate the neighbors for a new piece of data within a dataset we must first calculate the distance between each record in the dataset to the new piece of data. We can do this using our distance function prepared above. Once distances are calculated, we must sort all of the records in the training dataset by their distance to the new data. We can then select the top k to return as the most similar neighbors. We can do this by keeping track of the distance for each record in the dataset as a tuple, sort the list of tuples by the distance (in descending order) and then retrieve the neighbors.\n",
    "\n",
    "Step 3: Make Predictions, - The most similar neighbors collected from the training dataset can be used to make predictions.In the case of classification, we can return the most represented class among the neighbors. We can achieve this by performing the argmax() function on the list of output values from the neighbors. Given a list of class values observed in the neighbors, the argmax() function takes a set of unique class values and calls the count on the list of class values for each class value in the set.\n",
    "\n",
    "Step 4: Measure Accuracy- we are tried to show accuracy levels with different numbers of eigenvectors for pca and lda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean(p1, p2):\n",
    "    return np.sqrt(np.sum((p1-p2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train, test_point, k=5):\n",
    "    \n",
    "    distances = [] # Contains list of tuples (distance, label\n",
    "    \n",
    "    for data_point, label in zip(X_train, y_train):\n",
    "        distances.append((euclidean(test_point, data_point), label))\n",
    " \n",
    "    \n",
    "    sorted_distances = sorted(distances, key=lambda x: x[0])\n",
    "    k_nearest_neighbors = np.array(sorted_distances[:k])\n",
    "    freq = np.unique(k_nearest_neighbors[:,1], return_counts=True)\n",
    "    labels, counts = freq\n",
    "    majority_vote = labels[counts.argmax()]\n",
    "    return majority_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(X_test, y_test, X_train, y_train, k=5):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for test_point in X_test:\n",
    "        \n",
    "        pred_label = knn(X_train, y_train, test_point, k)\n",
    "        predictions.append(pred_label)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    accuracy = (predictions == y_test).sum() / y_test.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_pca= []\n",
    "\n",
    "for pca in result_pca:\n",
    "    X_test, y_test, X_train, y_train=prepareDataForTrainingTesting(pca)\n",
    "    accuracy=calculate_accuracy(X_test, y_test, X_train, y_train, k=5)\n",
    "    accuracy_list_pca.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_lda = []\n",
    "for lda in result_lda:\n",
    "    X_test, y_test, X_train, y_train=prepareDataForTrainingTesting(lda)\n",
    "    accuracy=calculate_accuracy(X_test, y_test, X_train, y_train, k=5)\n",
    "    accuracy_list_lda.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i =0\n",
    "for accyracy in accuracy_list_pca:\n",
    "    print(\"When the eigenvector value {} the accuracy result {}%\".format(K[i],accyracy*100))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(K, accuracy_list_pca)\n",
    "plt.title('Accuracy vs Eigenvector on PCA')\n",
    "plt.xlabel('Number of Eigenvector used')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i =0\n",
    "for accyracy in accuracy_list_lda:\n",
    "    print(\"When the eigenvector value {} the accuracy result {}%\".format(K[i],accyracy*100))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(K, accuracy_list_lda)\n",
    "plt.title('Accuracy vs Eigenvector on LDA')\n",
    "plt.xlabel('Number of Eigenvector used')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
